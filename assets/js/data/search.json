[ { "title": "[AWS][ECS] What I learned about CPU and memory management of Amazon ECS", "url": "/posts/ECS-CPU-Memory/", "categories": "CSIE, Cloud, AWS", "tags": "aws, ecs, memory, cpu", "date": "2022-05-08 01:06:00 +0100", "snippet": "IntroductionThis article will go through what I learned from How Amazon ECS manages CPU and memory resources and few things I add about the CPU and memory management of Amazon ECS.Two General RulesThe blog1 mentioned that there are two general rules of thumb with containers: unless otherwise restricted and capped, a container gets access to all the CPU and memory capacity available on a given host (operating system). unless otherwise protected and guaranteed, all containers share CPU, memory, and other resources on a given host (operating system) in the same way that other processes running on that host share those resources.And we are going to talk about how the task and container access and share the CPU and memory resources with the related configuration defined in a task definition.MemoryFor memory management, ECS provides the configuration of two levels: container-level: memoryReservation (soft limit) and memory (hard limit) task-level: memory (hard limit)Let’s discuss what the differences, how these settings affect task scheduling and some scenarios of these different memory setting.Container-level: memoryReservation(soft limit) and memory(hard limit)memoryReservation(soft limit)The task definition document about memory setting mentioned that memoryReservation is a soft limit (in MiB): This option indicates the memory size that a task needs on a container instance when being placed. The sum of the memoryReservation of all containers in all the tasks running on a container instance, cannot exceed the available memory on that container instance.memory(hard limit)The task definition document about memory setting mentioned memory is the hard limit (in MiB) of the container: memory is the upper bound that the memory usage a container cannot go beyond. If the container exceed the memory, the container will be killed (by out-of-memory(OOM) killer of the Linux kernel which we will discuss later).Notes for memoryReservation(soft limit) and memory(hard limit)For the available memory of a container instance: If you specify memoryReservation, the available memory of a container instance will be subtracted by memoryReservation. If you do not specify memoryReservation, the available memory of a container instance will be subtracted by memory.See below tables for the reservation and ceiling of the memory size a container can use: Configuration Reservation Ceiling memoryReservation only memoryReservation total memory of the container instance or task memory size if configured memoryReservation and memory(memory must be greater than memoryReservation) memoryReservation memory memory only memory memory no memoryReservation nor memory none task-level memory size (and task-level memory size has to be configured, see below note) note: If a task-level memory size is not specified, you must specify a non-zero integer for one or both of memory or memoryReservation in a container definition.Out-Of-Memory(OOM) killerIf a container exceed the memory(hard limit), the container will be killed by Out-Of-Memory(OOM) killer. OOM killer is a Linux kernel mechanism to make sure that the system still have enough memory to run.2How the OOM killer choose the container to kill will not be discussed in this article. Nevertheless, the blog1 does mention below: If something is above its soft limit, it’s more likely to be killed than something below its soft limit, but figuring out which process gets killed requires knowing all the other processes on the system and what they are doing with their memory as well.As we have discussed that the ceiling or hard limit can be the container-level memory, task-level memory size or the total memory of the container instance, the mechanisms behind these limits are different.When the system has low memory and the OOM killer is triggered, we can see the below OOM messages from system logs:[11111111.111111] Out of memory: Kill process 11111 (java) score 311 or sacrifice childOn the other hand, the container-level and task-level limit is implemented by Linux control groups(cgroup)3 with which we can control the resources a process can utilised. If a container exceed the memory hard limit, we can see cgroup oom from the system logs:[2222222.222222] Memory cgroup out of memory: Kill process 22222 (nginx) score 0 or sacrifice childThe OOM usually results in the container to be stopped with exited code 137.We can take different actions based on if it is an OS-level OOM or a cgroup oom: For OS-level OOM, we can distribute the loading on different instances (like task placement) configure task-level or container-level memory limit(hard limit) For cgroup OOM, we can increase the task-level or container-level memory limit(hard limit) scale out the container to distribute the loading on more containers Swap spaceTo decreased the impact of low-memory issue, Linux has a mechanism called swap space to allow OS to swap the physical memory to disk to free up memory space to use.Docker also provide the option to allow container to use swap4 and ECS introduce the parameter to configure swap space for each container5. In this way, even the container may hit the hard limit, the container can use swap space to avoid triggering OOM killer.The blog1 mention below: While swap config allows the container to swap if configured properly, it does not include accounting for swap usage in the scheduling phase; swap is not treated as a resource for the ECS scheduler.…While the OOM behavior isn’t changing, now containers can be configured to swap out to disk in a memory pressure scenario. This can potentially alleviate the need for the OOM killer to kick in (if containers are configured to swap).Task-level: memory limit (hard limit)We already mentioned that we can configure the task memory size and if the total memory usage of the containers within the task exceed the task memory size, OOM killer will be triggered.When configuring the container-level and task-level memory, we will have the following notices as mentioned by the blog1 : none of the containers can have a memory hard limit that exceeds the memory size of the task the sum of all hard limits can exceed task-level memory size the sum of all soft limits cannot exceed task-level memory sizeScenarios for different memory configurationsWith the information above, let’s assume the following situations happen in your environment. The task size for memory is 512MB. Container memory configuration: Container Hard limit Essential container A 300 MB Yes B 300 MB No If the essential parameter of a container is marked as true, and that container fails or stops for any reason, all other containers that are part of the task are stopped.6Scenario 1: Container A reach hard limit The total memory usage for container A and B do not reach the task size for memory.The container A will be killed by OOM, and the task will be stopped.Scenario 2 Container B reach hard limit The total memory usage for container A and B do not reach the task size for memory.The container B will be killed by OOM, and the task will not be stopped.Scenario 3: Container A and B does not reach hard limit The task size for memory usage has been reached.It may result in container A or B to be stopped by OOM, depending on which process in container A or B has the highest OOM score.Of course, if the stopped container is Container A, as Container A is an essential container, the task will be stopped correspondingly.CPUECS also provide different levels of CPU usage: container-level and task-level. And the mechanism is different from how the memory limit works.vCPU and CPU unitECS introduce an abstract term call CPU unit. The blog1 mentions below: “CPU units” is just an ECS construct and it doesn’t exist in the world of the Linux kernel…. As a baseline ECS considers each vCPU available to an EC2 container instance as 1024 units. That is, an EC2 host that has 8 vCPUs has 8×1024=8192 units available.Container-level: a relative shareThe container-level CPU limit acts as a relative share or weight: Take the example from the blog1 :A task running on a host with 8192 CPU units has two containers: containerA: assigned with 512 units containerB: assigned with 1024 unitsWhen both containers are busy and under high loading, they can access the rest CPU time (8192-512-1024=6656 CPU units). The rest of CPU time will be shared as: containerA can get access to 1/3 of 6656 = 2219 CPU units containerB can get access to 2/3 of 6656= 4437 CPU unitsThe blog1 describe the following regarding CPU usages: if containers are not using their allotted CPU units, other containers can use that capacity. When capacity is not used, any container can burst to use that spare capacity. CPU shares control the amount of CPU capacity available when there is CPU contention; that is, multiple containers attempting to use the CPU at the same time. When you don’t specify any CPU units for a container, ECS intrinsically enforces two Linux CPU shares(which we will discuss later below) for the cgroup (which is the minimum allowed).Task-level: hard limitThe task-level CPU setting is different from container-level CPU setting. The task-level CPU setting is an upper bound of the CPU limit the containers can use.If the container exceeds the task-level CPU limit, the container will not be killed like a container will be killed by OOM when exceeding memory hard limit.Scenarios for different CPU configurationsWe can do the following test on a container with 2 vCPUs with and without task-level and different container-level CPU limit to see how it will turn out.Scenario 1: Without task-level CPU limitThere are two containers with the container-level setting: 256 CPU units and 512 CPU units. We can see that the total CPU utilisation is two vCPU and the CPU shared is based on the ratio of the container-level CPU setting:$ docker statsCONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS5cdf7ea5edf7 ecs-agent 0.06% 14.91MiB / 7.544GiB 0.19% 0B / 0B 0B / 0B 13391f208f0606 ecs-stressWithoutTaskCpu-5-stress256-848ebdebcea0ebe8aa01 66.57% 956KiB / 7.544GiB 0.01% 836B / 0B 0B / 0B 3c73331a916f3 ecs-stressWithoutTaskCpu-5-stress512-86bbd0e094d6e6cdb801 131.23% 880KiB / 7.544GiB 0.01% 836B / 0B 0B / 0B 3Scenario 2: With task-level CPU setting: 1024 CPU unitsThere are two containers with the container-level setting: 256 CPU units and 512 CPU units. We can see that the total CPU utilisation is one vCPU and the CPU shared is based on the ratio of the container-level CPU setting:$ docker statsCONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS5cdf7ea5edf7 ecs-agent 0.00% 14.92MiB / 7.544GiB 0.19% 0B / 0B 0B / 0B 1320bc598752c7 ecs-stressWithTaskCpu-2-stress512-98b3befeffe68196de01 67.19% 832KiB / 7.544GiB 0.01% 906B / 0B 0B / 0B 3487ef173aca2 ecs-stressWithTaskCpu-2-stress256-bafab0898b96d5d3b701 33.52% 924KiB / 7.544GiB 0.01% 816B / 0B 0B / 0B 3Under the hoodMemoryAs the container resource limit is implemented by cgroup, we can actually check the memory limit from host-level and check the cgroup configuration.cgroup setting for memoryFrom the cgroup document7, we can see that the following files regarding memory limit: memory.soft_limit_in_bytes: set/show soft limit of memory usage memory.limit_in_bytes: set/show limit of memory usageLet’s have a ECS task 602081644df34fb5bf1c5786ef261fe7 which is configured to have 1024 MiB task-level memory setting and the container-level has 128MiB soft limit. We can find these memory limit from cgroup settings.Task-level memory limitWe can find a directory of cgroup match the task ID and we can see that the memory.limit_in_bytes is around 1024 MiB:$ cat /sys/fs/cgroup/memory/ecs/602081644df34fb5bf1c5786ef261fe7/memory.limit_in_bytes1073741824Container-level memory limitUnder the directory of cgroup for the ECS task, we will find another directory the and directory name will match the container ID of the ECS task. We can further check the memory.soft_limit_in_bytes and see that the value is around 128 MiB. Since I didn’t set the container-level hard limit, we can see memory.limit_in_bytes is a very huge number:# soft limit 128 MB$ cat /sys/fs/cgroup/memory/ecs/602081644df34fb5bf1c5786ef261fe7/2624b365b6ea72e9c9a9825038136f30c9721665c0e6394fabf29c6960edb593/memory.soft_limit_in_bytes134217728$ cat /sys/fs/cgroup/memory/ecs/602081644df34fb5bf1c5786ef261fe7/2624b365b6ea72e9c9a9825038136f30c9721665c0e6394fabf29c6960edb593/memory.limit_in_bytes9223372036854771712CPUcgroup setting for CPUcgroup setting for CPU has different concept from memory. RedHat document gives a detail description about the cgroup setting for CPU8: cpu.shares contains an integer value that specifies a ==relative share== of CPU time available to the tasks in a cgroup. For example, tasks in two cgroups that have cpu.shares set to 100 will receive equal CPU time, but tasks in a cgroup that has cpu.shares set to 200 receive twice the CPU time of tasks in a cgroup where cpu.shares is set to 100. The value specified in the cpu.shares file must be 2 or higher. cpu.cfs_period_us specifies a period of time in microseconds (µs, represented here as “us”) for how regularly a cgroup’s access to CPU resources should be reallocated. cpu.cfs_quota_us specifies the total amount of time in microseconds (µs, represented here as “us”) for which all tasks in a cgroup can run during one period (as defined by cpu.cfs_period_us). nr_throttled: number of times tasks in a cgroup have been throttled (that is, not allowed to run because they have exhausted all of the available time as specified by their quota). throttled_time: the total time duration (in nanoseconds) for which tasks in a cgroup have been throttled.Container-level limitLet’s have a container without any container-level CPU setting:$ ls -l /sys/fs/cgroup/cpu/ecs/4ed351a8c26d47f0bb502c7019256bc7/5934355a9835e1ea5a70333ffbfeab6767ae046cdf05748b8083640f675f46db/total 0-rw-r--r-- 1 root root 0 May 3 21:39 cgroup.clone_children-rw-r--r-- 1 root root 0 May 3 21:39 cgroup.procs-r--r--r-- 1 root root 0 May 3 21:39 cpuacct.stat-rw-r--r-- 1 root root 0 May 3 21:39 cpuacct.usage-r--r--r-- 1 root root 0 May 3 21:39 cpuacct.usage_all-r--r--r-- 1 root root 0 May 3 21:39 cpuacct.usage_percpu-r--r--r-- 1 root root 0 May 3 21:39 cpuacct.usage_percpu_sys-r--r--r-- 1 root root 0 May 3 21:39 cpuacct.usage_percpu_user-r--r--r-- 1 root root 0 May 3 21:39 cpuacct.usage_sys-r--r--r-- 1 root root 0 May 3 21:39 cpuacct.usage_user-rw-r--r-- 1 root root 0 May 3 21:39 cpu.cfs_period_us-rw-r--r-- 1 root root 0 May 3 21:39 cpu.cfs_quota_us-rw-r--r-- 1 root root 0 May 3 21:39 cpu.rt_period_us-rw-r--r-- 1 root root 0 May 3 21:39 cpu.rt_runtime_us-rw-r--r-- 1 root root 0 May 3 21:39 cpu.shares-r--r--r-- 1 root root 0 May 3 21:39 cpu.stat-rw-r--r-- 1 root root 0 May 3 21:39 notify_on_release-rw-r--r-- 1 root root 0 May 3 21:39 tasksWe can see that the cpu.shares is the default value 2:$ cat /sys/fs/cgroup/cpu/ecs/4ed351a8c26d47f0bb502c7019256bc7/5934355a9835e1ea5a70333ffbfeab6767ae046cdf05748b8083640f675f46db/cpu.shares2cpu.cfs_period_us 100000 and cpu.cfs_quota_us -1 means that the container can access the CPU every 1 second and there is no restriction on accessing CPU9. Since there is no hard-limit, there is no throttling happening:$ cat /sys/fs/cgroup/cpu/ecs/4ed351a8c26d47f0bb502c7019256bc7/5934355a9835e1ea5a70333ffbfeab6767ae046cdf05748b8083640f675f46db/cpu.cfs_period_us100000$ cat /sys/fs/cgroup/cpu/ecs/4ed351a8c26d47f0bb502c7019256bc7/5934355a9835e1ea5a70333ffbfeab6767ae046cdf05748b8083640f675f46db/cpu.cfs_quota_us-1$ cat /sys/fs/cgroup/cpu/ecs/4ed351a8c26d47f0bb502c7019256bc7/5934355a9835e1ea5a70333ffbfeab6767ae046cdf05748b8083640f675f46db/cpu.statnr_periods 0nr_throttled 0throttled_time 0Task-level limitFor example, I have an ECS task a3d6aa8e76e540a0b48e9ac162c75adb with task-level CPU setting 512 CPU units, I can check the CPU limit in the following path:$ aws ecs list-tasks --cluster ECS-Test | grep 4ed &quot;arn:aws:ecs:us-east-1:111111111111:task/ECS-Test/4ed351a8c26d47f0bb502c7019256bc7&quot;,$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5934355a9835 httpd &quot;httpd-foreground&quot; 13 days ago Up 13 days 0.0.0.0:49153-&amp;gt;80/tcp, :::49153-&amp;gt;80/tcp ecs-apache-20-apache-a0d8c096ccc599997e00$ ls -l /sys/fs/cgroup/cpu/ecstotal 0drwxr-xr-x 3 root root 0 Apr 19 22:33 4ed351a8c26d47f0bb502c7019256bc7$ ls -l /sys/fs/cgroup/cpu/ecs/4ed351a8c26d47f0bb502c7019256bc7/total 0drwxr-xr-x 2 root root 0 Apr 19 22:33 5934355a9835e1ea5a70333ffbfeab6767ae046cdf05748b8083640f675f46db-rw-r--r-- 1 root root 0 May 3 21:39 cgroup.clone_children-rw-r--r-- 1 root root 0 May 3 21:39 cgroup.procs-r--r--r-- 1 root root 0 May 3 21:39 cpuacct.stat-rw-r--r-- 1 root root 0 May 3 21:39 cpuacct.usage-r--r--r-- 1 root root 0 May 3 21:39 cpuacct.usage_all-r--r--r-- 1 root root 0 May 3 21:39 cpuacct.usage_percpu-r--r--r-- 1 root root 0 May 3 21:39 cpuacct.usage_percpu_sys-r--r--r-- 1 root root 0 May 3 21:39 cpuacct.usage_percpu_user-r--r--r-- 1 root root 0 May 3 21:39 cpuacct.usage_sys-r--r--r-- 1 root root 0 May 3 21:39 cpuacct.usage_user-rw-r--r-- 1 root root 0 Apr 19 22:33 cpu.cfs_period_us-rw-r--r-- 1 root root 0 Apr 19 22:33 cpu.cfs_quota_us-rw-r--r-- 1 root root 0 May 3 21:39 cpu.rt_period_us-rw-r--r-- 1 root root 0 May 3 21:39 cpu.rt_runtime_us-rw-r--r-- 1 root root 0 May 3 21:39 cpu.shares-r--r--r-- 1 root root 0 May 3 21:39 cpu.stat-rw-r--r-- 1 root root 0 May 3 21:39 notify_on_release-rw-r--r-- 1 root root 0 May 3 21:39 taskscpu.cfs_period_us 100000 and cpu.cfs_quota_us 50000 means that the container can access the CPU for 0.5 second for every 1 second.$ cat /sys/fs/cgroup/cpu/ecs/4ed351a8c26d47f0bb502c7019256bc7/cpu.cfs_period_us100000$ cat /sys/fs/cgroup/cpu/ecs/4ed351a8c26d47f0bb502c7019256bc7/cpu.cfs_quota_us50000We can also see that the container has metrics indicating that throttling happened:$ cat /sys/fs/cgroup/cpu/ecs/4ed351a8c26d47f0bb502c7019256bc7/cpu.statnr_periods 140596nr_throttled 7throttled_time 388807946SummaryThe AWS blog1 does explain in detail about the memory and CPU management of Amazon ECS and I just organized in a way I understand the blog. Hope this article can give you a brief understanding about how Amazon ECS design and implement the management of memory and CPU management.References How Amazon ECS manages CPU and memory resources &amp;#8617; &amp;#8617;2 &amp;#8617;3 &amp;#8617;4 &amp;#8617;5 &amp;#8617;6 &amp;#8617;7 &amp;#8617;8 Out Of Memory Management &amp;#8617; cgroups(7) - Linux manual page &amp;#8617; Runtime options with Memory, CPUs, and GPUs &amp;#8617; Amazon ECS Now Supports Per-Container Swap Space Parameters &amp;#8617; Task definition parameters - Amazon Elastic Container Service &amp;#8617; https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt &amp;#8617; 3.2. cpu Red Hat Enterprise Linux 6 &amp;#8617; https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt &amp;#8617; " }, { "title": "[AWS][ECS] Difference between storage_stats and blkio_stats from ECS metadata endpoint", "url": "/posts/Difference-storage_states-and-blkio_stats/", "categories": "CSIE, Cloud, AWS", "tags": "ecs, windows", "date": "2022-04-19 00:30:00 +0100", "snippet": "Introduction / SummaryThis article will go though a brief explanation that why storage_stats and blkio_stats have different output from ECS metadata endpoint.ScenarioSome people will question that why the field storage_stats is empty from ECS task metadata while blkio_stats has metrics. People expect that the field storage_stats should provide some metrics about storage.ExplanationAs ECS agent get the metrics from docker ContainerStats API, we can further check the metrics from moby repository.From the following code, we can see that storage_stats is for Windows and blkio_stats is for Linux.moby code: // Linux specific stats, not populated on Windows.PidsStats PidsStats `json:&quot;pids_stats,omitempty&quot;`BlkioStats BlkioStats `json:&quot;blkio_stats,omitempty&quot;`// Windows specific stats, not populated on Linux.NumProcs uint32 `json:&quot;num_procs&quot;`StorageStats StorageStats `json:&quot;storage_stats,omitempty&quot;`moby code:// StorageStats is the disk I/O stats for read/write on Windows.type StorageStats struct { ReadCountNormalized uint64 `json:&quot;read_count_normalized,omitempty&quot;` ReadSizeBytes uint64 `json:&quot;read_size_bytes,omitempty&quot;` WriteCountNormalized uint64 `json:&quot;write_count_normalized,omitempty&quot;` WriteSizeBytes uint64 `json:&quot;write_size_bytes,omitempty&quot;`}LabECS task on WindowsAs I only saw that storage_stats is for Windows from the code comments, I need to confirm this behaviour. Therefore, I launched an ECS task on Windows and accessed the ECS metadata endpoint from host level. We can see that storage_stats has value and blkio_stats has no value:# on windowsInvoke-WebRequest -uri &quot;http://169.254.170.2/v4/2239d0bc-e1b9-48e0-9d03-f44a30935f9f/stats&quot; -UseBasicParsing | Select-Object -Expand Content{ &quot;read&quot;: &quot;2022-03-03T15:06:37.9523371Z&quot;, &quot;preread&quot;: &quot;2022-03-03T15:06:36.9398213Z&quot;, &quot;pids_stats&quot;: {}, &quot;blkio_stats&quot;: { &quot;io_service_bytes_recursive&quot;: null, &quot;io_serviced_recursive&quot;: null, &quot;io_queue_recursive&quot;: null, &quot;io_service _time_recursive&quot;: null, &quot;io_wait_time_recursive&quot;: null, &quot;io_merged_recursive&quot;: null, &quot;io_time_recursive&quot;: null, &quot;sectors_recursive&quot;: null }, &quot;num_procs&quot;: 4, &quot;storage_stats&quot;: { &quot;read_count_normalized&quot;: 46167, &quot;read_size_bytes&quot;: 32.0026112, &quot;write_count_normalized&quot;: 2982, &quot;write_size_bytes&quot;: 22934016 },...While an output from ECS metadata endpoint for ECS task on Linux may be like the following that the storage_stats is empty and blkio_stats has value:$ curl ${ECS_CONTAINER_METADATA_URI_V4}/task/stats&quot;blkio_stats&quot;: { &quot;io_service_bytes_recursive&quot;: [ { &quot;major&quot;: 259, &quot;minor&quot;: 0, &quot;op&quot;: &quot;Read&quot;, &quot;value&quot;: 0 }, { &quot;major&quot;: 259, &quot;minor&quot;: 0, &quot;op&quot;: &quot;Write&quot;, &quot;value&quot;: 20992 },... &quot;num_procs&quot;: 0, &quot;storage_stats&quot;: {},AWS Document for ECS task on WindowsWe can also see that storage_stats has metrics in the test stats for windows from ECS agent GitHub repository:{ &quot;blkio_stats&quot;: { &quot;io_service_bytes_recursive&quot;: null, &quot;io_serviced_recursive&quot;: null, &quot;io_queue_recursive&quot;: null, &quot;io_service_time_recursive&quot;: null, &quot;io_wait_time_recursive&quot;: null, &quot;io_merged_recursive&quot;: null, &quot;io_time_recursive&quot;: null, &quot;sectors_recursive&quot;: null }, &quot;num_procs&quot;: 8, &quot;storage_stats&quot;: { &quot;read_count_normalized&quot;: 1, &quot;read_size_bytes&quot;: 3, &quot;write_count_normalized&quot;: 1, &quot;write_size_bytes&quot;: 15 },SummaryAs people may use other monitoring tool like datadog to access ECS metadata point to get resource usage metrics and question why the field is empty1, we can see that it is expected to see the field storage_stats to be empty if the container is not on Windows.References storage_stats is empty · Issue #2935 · aws/amazon-ecs-agent &amp;#8617; " }, { "title": "[AWS][ECS] How to calculate CPU utilization from ECS endpoint?", "url": "/posts/ECS-calculate-CPU-utilization-metadata-endpoing/", "categories": "CSIE, AWS", "tags": "ecs, cpu", "date": "2022-04-17 00:01:11 +0100", "snippet": "Introduction / SummaryThis article will go though how to check and calculate the container CPU utilization from ECS metadata endpoint.ScenarioSometimes people want to know how to calculate the performance usages of the containers. They can use sidecar container to retrieve the data from the metrics provided by the ECS metadata endpoint. Thus they can collect, calculate and export the performance metrics like CPU utilization.ExplanationThe CPU metrics provided by ECS metadata endpointThe ECS metadata document mentions the following metadata path regarding the resource usage of container/task: ${ECS_CONTAINER_METADATA_URI_V4}/stats: This path returns Docker stats for the specific container. ${ECS_CONTAINER_METADATA_URI_V4}/task/stats : This path returns Docker stats for all of the containers associated with the task.By accessing the endpoint ${ECS_CONTAINER_METADATA_URI_V4}/stats inside a container, we can see the following output about CPU statistic:$ curl ${ECS_CONTAINER_METADATA_URI_V4}/stats | jq &#39;.cpu_stats&#39;{ &quot;cpu_usage&quot;: { &quot;total_usage&quot;: 6854309182, &quot;percpu_usage&quot;: [ 3836064097, 3018245085 ], &quot;usage_in_kernelmode&quot;: 3510000000, &quot;usage_in_usermode&quot;: 2790000000 }, &quot;system_cpu_usage&quot;: 1139128190000000, &quot;online_cpus&quot;: 2, &quot;throttling_data&quot;: { &quot;periods&quot;: 0, &quot;throttled_periods&quot;: 0, &quot;throttled_time&quot;: 0 }}We can see another field called precpu_stats. The Docker API ContainerStats document mentioned that the precpu_stats is the CPU statistic of the previous read, and is used to calculate the CPU usage percentage. precpu_stats is different from the cpu_stats field.$ curl ${ECS_CONTAINER_METADATA_URI_V4}/stats | jq &#39;.precpu_stats&#39;{ &quot;cpu_usage&quot;: { &quot;total_usage&quot;: 6854629990, &quot;percpu_usage&quot;: [ 3836187199, 3018442791 ], &quot;usage_in_kernelmode&quot;: 3510000000, &quot;usage_in_usermode&quot;: 2790000000 }, &quot;system_cpu_usage&quot;: 1139150390000000, &quot;online_cpus&quot;: 2, &quot;throttling_data&quot;: { &quot;periods&quot;: 0, &quot;throttled_periods&quot;: 0, &quot;throttled_time&quot;: 0 }}How to calculate CPU utilization?As ECS metadata document mentions that these data are retrieved via Docker API ContainerStats , we can see the Docker API ContainerStats document mention how to calculate the CPU uilization: cpu_delta = cpu_stats.cpu_usage.total_usage - precpu_stats.cpu_usage.total_usage system_cpu_delta = cpu_stats.system_cpu_usage - precpu_stats.system_cpu_usage number_cpus = lenght(cpu_stats.cpu_usage.percpu_usage) or cpu_stats.online_cpus CPU usage % = (cpu_delta / system_cpu_delta) * number_cpus * 100.0With the example above, we can then calculate the average CPU utilization: cpu_delta = cpu_stats.cpu_usage.total_usage - precpu_stats.cpu_usage.total_usage cpu_delta = 6854309182 - 6854629990 system_cpu_delta = cpu_stats.system_cpu_usage - precpu_stats.system_cpu_usage system_cpu_delta = 1139128190000000 - 1139150390000000 number_cpus = lenght(cpu_stats.cpu_usage.percpu_usage) or cpu_stats.online_cpus number_cpus = 2 CPU usage % = (cpu_delta / system_cpu_delta) * number_cpus * 100.0 CPU usage % = (6854309182 - 6854629990) / (1139128190000000 - 1139150390000000) * 2 * 100 Is system_cpu_usage considered as “shared” or “separated” for the containers ?Some people are wondering if the system_cpu_usage is the same for each container stat output or the metrics is different for each container. For this question, we can check the related code, I can see that the system_cpu_usage is system-level: Find the system_cpu_usage: code// System Usage. Linux only.SystemUsage uint64 `json:&quot;system_cpu_usage,omitempty&quot;` Find that getSystemCPUUsage will return the CPU usage: code// Sample system CPU usage close to container usage to avoid// noise in metric calculations.systemUsage, err := s.getSystemCPUUsage()if err != nil { logrus.WithError(err).WithField(&quot;container_id&quot;, pair.container.ID).Errorf(&quot;collecting system cpu usage&quot;) continue} Confirm the system CPU usage is retrieved from /proc/stat: codefunc (s *Collector) getSystemCPUUsage() (uint64, error) { f, err := os.Open(&quot;/proc/stat&quot;)...... /proc/stat is kernel/system-wise static: proc(5) - Linux manual page[root@ip-172-31-83-85 ~]# cat /proc/statcpu 1858672 145 937683 147711096 42100 0 8871 790 0 0cpu0 948377 51 470796 73832328 20560 0 5919 200 0 0cpu1 910294 94 466886 73878768 21539 0 2951 589 0 0 Confirm that system_cpu_usage is the same for both containers in the same task by accessing ${ECS_CONTAINER_METADATA_URI_V4}/task/stats:$ curl ${ECS_CONTAINER_METADATA_URI_V4}/task/stats &quot;5f71e2b846442b1cc146d399135f490fdbbb5f3936b38119ffcf3c5d85308ac5&quot;: { &quot;cpu_stats&quot;: {... &quot;system_cpu_usage&quot;: 1478570440000000,... &quot;b0f919cbff4c787ae765cbcc8b094d4375bdb002848d78e9e635356bd94d8484&quot;: { &quot;cpu_stats&quot;: {... &quot;system_cpu_usage&quot;: 1478570450000000,...We can see system_cpu_usage is the system-level metrics and this metric will be the same for both containers.SummaryECS metadata endpoint provides metrics of container resources so people have the flexibility to collect, calculate the container performance by their own application or other third-party application like datadog.We can also see that if we want to know deeper about the metrics, we can dig into the code and find the metrics resources to get more insight about the metrics." }, { "title": "[AWS][CLB] Which AZ will CLB scale?", "url": "/posts/CLB-Scaling-in-AZ/", "categories": "CSIE, Cloud, AWS", "tags": "elb, clb, aws, scaling, networking", "date": "2022-01-02 17:20:24 +0000", "snippet": "IntroductionWhen under the following conditions: CLB enables multiple AZs. Cross-zone load balancing is disabled. Healthy backend instances only locate in one of the AZs.CLB DNS only register the IPs of the AZ where the healthy backend instance locate.1 Therefore, some people may wonder CLB will scale: All enabled AZ? AZ with backends? Or AZ with healthy backends?This article records a simple lab for CLB scaling under the conditions mentioned above and demonstrate that the CLB scales in the enabled AZ with any backend instance no matter the backend instance is healthy or not.ScenariosIn these three different scenarios below: Scenario #1: test if CLB only scales in AZ with healthy backends AZ enabled if backend instance exist backend health us-east-1a Yes health us-east-1b Yes unhealthy Scenario #2: test if CLB scales in all enabled AZ AZ enabled if backend instance exist backend health us-east-1a No none us-east-1b Yes unhealthy us-east-1c Yes health Scenario #3: test if CLB scales in AZ with backends AZ enabled if backend instance exist backend health us-east-1a Yes health us-east-1b Yes unhealthy us-east-1c Yes health I will do use ab2 to increase the load on CLB to let CLB scale and check which AZ will the CLB scales.Scenario #1: test if CLB only scales in AZ with healthy backends AZ enabled if backend instance exist backend health us-east-1a Yes health us-east-1b Yes unhealthy Check the backend instances: $ aws ec2 describe-instances \\ --instance-ids $(aws elb describe-instance-health --load-balancer-name CLB-Test --query &#39;InstanceStates[].InstanceId&#39; --output text) \\ --query &#39;Reservations[*].Instances[*].[PrivateDnsName,Placement.AvailabilityZone]&#39; \\ --output table-------------------------------------------------| DescribeInstances |+--------------------------------+--------------+| ip-172-31-33-197.ec2.internal | us-east-1a || ip-172-31-12-183.ec2.internal | us-east-1b |+--------------------------------+--------------+ Check the ENIs for CLB: $ aws ec2 describe-network-interfaces | jq -r &#39;.NetworkInterfaces[] | select(.Description | contains(&quot;CLB-Test&quot;)) | {IP:.Association.PublicIp, AZ:.AvailabilityZone}&#39;{ &quot;IP&quot;: &quot;3.212.184.186&quot;, &quot;AZ&quot;: &quot;us-east-1b&quot;}{ &quot;IP&quot;: &quot;52.54.181.219&quot;, &quot;AZ&quot;: &quot;us-east-1a&quot;} Confirm only the IP of the AZ with healthy backend in CLB DNS record: $ dig +short CLB-Test-1513212108.us-east-1.elb.amazonaws.com52.54.181.219 Use ab command to test CLB. See the CLB scaled in both AZs no matter the backend is healthy or not: $ aws ec2 describe-network-interfaces | jq -r &#39;.NetworkInterfaces[] | select(.Description | contains(&quot;CLB-Test&quot;)) | {IP:.Association.PublicIp, AZ:.AvailabilityZone}&#39;{ &quot;IP&quot;: &quot;3.214.48.44&quot;, &quot;AZ&quot;: &quot;us-east-1b&quot;}{ &quot;IP&quot;: &quot;3.212.184.186&quot;, &quot;AZ&quot;: &quot;us-east-1b&quot;}{ &quot;IP&quot;: &quot;52.54.213.202&quot;, &quot;AZ&quot;: &quot;us-east-1a&quot;}{ &quot;IP&quot;: &quot;52.54.181.219&quot;, &quot;AZ&quot;: &quot;us-east-1a&quot;} Scenario #2: test if CLB scales in all enabled AZ AZ enabled if backend instance exist backend health us-east-1a No none us-east-1b Yes unhealthy us-east-1c Yes health Check the backend instances: $ aws ec2 describe-instances \\ --instance-ids $(aws elb describe-instance-health --load-balancer-name CLB-Test --query &#39;InstanceStates[].InstanceId&#39; --output text) \\ --query &#39;Reservations[*].Instances[*].[PrivateDnsName,Placement.AvailabilityZone]&#39; \\ --output table-------------------------------------------------| DescribeInstances |+--------------------------------+--------------+| ip-172-31-86-255.ec2.internal | us-east-1c || ip-172-31-2-69.ec2.internal | us-east-1b |+--------------------------------+--------------+ Check the ENIs for CLB: $ aws ec2 describe-network-interfaces | jq -r &#39;.NetworkInterfaces[] | select(.Description | contains(&quot;CLB-Test&quot;)) | {IP:.Association.PublicIp, AZ:.AvailabilityZone}&#39;{ &quot;IP&quot;: &quot;3.210.180.211&quot;, &quot;AZ&quot;: &quot;us-east-1b&quot;}{ &quot;IP&quot;: &quot;3.230.196.107&quot;, &quot;AZ&quot;: &quot;us-east-1c&quot;}{ &quot;IP&quot;: &quot;52.86.145.124&quot;, &quot;AZ&quot;: &quot;us-east-1a&quot;} Confirm only the IP of the AZ with healthy backend in CLB DNS record: $ dig CLB-Test-1593697151.us-east-1.elb.amazonaws.com +short3.230.196.107 Use ab command to test CLB. See the CLB did not scale the enabled AZ without any backend which is us-east-1c in this lab: $ aws ec2 describe-network-interfaces | jq -r &#39;.NetworkInterfaces[] | select(.Description | contains(&quot;CLB-Test&quot;)) | {IP:.Association.PublicIp, AZ:.AvailabilityZone}&#39;{ &quot;IP&quot;: &quot;54.86.219.139&quot;, &quot;AZ&quot;: &quot;us-east-1c&quot;}{ &quot;IP&quot;: &quot;3.230.196.107&quot;, &quot;AZ&quot;: &quot;us-east-1c&quot;}{ &quot;IP&quot;: &quot;3.210.180.211&quot;, &quot;AZ&quot;: &quot;us-east-1b&quot;}{ &quot;IP&quot;: &quot;3.211.33.87&quot;, &quot;AZ&quot;: &quot;us-east-1b&quot;}{ &quot;IP&quot;: &quot;52.86.145.124&quot;, &quot;AZ&quot;: &quot;us-east-1a&quot;} Scenario #3: test if CLB scales in AZ with backends AZ enabled if backend instance exist backend health us-east-1a Yes health us-east-1b Yes unhealthy us-east-1c Yes health Follow the scenario #2, I add another backend instance to us-east-1a: $ aws ec2 describe-instances \\ --instance-ids $(aws elb describe-instance-health --load-balancer-name CLB-Test --query &#39;InstanceStates[].InstanceId&#39; --output text) \\ --query &#39;Reservations[*].Instances[*].[PrivateDnsName,Placement.AvailabilityZone]&#39; \\ --output table-------------------------------------------------| DescribeInstances |+--------------------------------+--------------+| ip-172-31-86-255.ec2.internal | us-east-1c || ip-172-31-33-197.ec2.internal | us-east-1a || ip-172-31-2-69.ec2.internal | us-east-1b |+--------------------------------+--------------+ See the CLB scale in us-east-1a as well: $ aws ec2 describe-network-interfaces | jq -r &#39;.NetworkInterfaces[] | select(.Description | contains(&quot;CLB-Test&quot;)) | {IP:.Association.PublicIp, AZ:.AvailabilityZone}&#39;{ &quot;IP&quot;: &quot;52.86.145.124&quot;, &quot;AZ&quot;: &quot;us-east-1a&quot;}{ &quot;IP&quot;: &quot;52.55.108.164&quot;, &quot;AZ&quot;: &quot;us-east-1a&quot;}{ &quot;IP&quot;: &quot;54.86.219.139&quot;, &quot;AZ&quot;: &quot;us-east-1c&quot;}{ &quot;IP&quot;: &quot;3.230.196.107&quot;, &quot;AZ&quot;: &quot;us-east-1c&quot;}{ &quot;IP&quot;: &quot;3.211.33.87&quot;, &quot;AZ&quot;: &quot;us-east-1b&quot;}{ &quot;IP&quot;: &quot;3.210.180.211&quot;, &quot;AZ&quot;: &quot;us-east-1b&quot;} SummaryTake a look of the labs above: Scenario #1: test if CLB only scales in AZ with healthy backends AZ enabled if backend instance exist backend health if CLB scale us-east-1a Yes health Yes us-east-1b Yes unhealthy Yes Scenario #2: test if CLB scales in all enabled AZ AZ enabled if backend instance exist backend health if CLB scale us-east-1a No none No us-east-1b Yes unhealthy Yes us-east-1c Yes health Yes Scenario #3: test if CLB scales in AZ with backends AZ enabled if backend instance exist backend health if CLB scale us-east-1a Yes health Yes us-east-1b Yes unhealthy Yes us-east-1c Yes health Yes From the labs above, we can see that the CLB scales in the enabled AZ with any backend instance no matter the backend instance is healthy or not.For CLB with cross-zone disabled, balancing the backends capacity in all the enable AZs is a better way as typically the requests on each IP of CLB may be roughly balancing (but it still depends on the behaviour of clients, name server, resolver).For CLB with cross-zone enabled, it may not be needed to balance the backend capacity in each AZ, but the client may experience cross-zone latency.3References How Elastic Load Balancing works - Request routing &amp;#8617; ab - Apache HTTP server benchmarking tool - Apache HTTP Server Version 2.4 &amp;#8617; Improving Performance and Reducing Cost Using Availability Zone Affinity - AWS Architecture Blog &amp;#8617; " }, { "title": "[Linux] Port range for NFS client", "url": "/posts/NFS-Client-Port-Range/", "categories": "CSIE, Linux", "tags": "linux, nfs, tcp, ip, firewall", "date": "2021-12-30 16:17:17 +0000", "snippet": "IntroductionThis article will go through how NFS client choose the port for communication with NFS server.ScenarioNetwork File System (NFS) is a protocol to allow a client to access files on a remote server over a network like local storage.1 To secure and restrict the access from clients to the files on a remote server, a administrator usually will use firewall to limit the access resources.On AWS, VPC provides several firewall configurations, the basic firewall configurations are the followings: security group:2 a stateful firewall at the network interface(ENI) level network access control list (ACL):3 a stateless firewall at the subnet level. Issue DescriptionWhen using Amazon Elastic File System (Amazon EFS), which support NFS, on the EFS side, it requires the configuration of security groups and the security groups should allow inbound access for the TCP protocol on the NFS port from the clients (EC2 instances).4 However, if we configure NACL on the subnets of EFS, we not only need to allow the inbound traffic from the clients to the port 2049(NFS port), but we also need to consider the outbound traffic to the client from EFS back to the clients.For NACL, we need to know what the port range the NFS client will use for communication with NFS server, or we will get timeout if the NACL does not allow the traffic from EFS to the clients.ExplanationEphemeral portTypically, when a client communicate with service on a server, each transport communication (or network socket) is identified by the five-tuple(or socket address): Protocol Source Address Destination Address Source Port Destination PortServices are usually located at fixed, “well-known” ports, for example, HTTP services usually listen on port 80 and HTTPS services usually listen on port 443. For client side, no fixed port is defined and a port is chosen automatically by the client. Ports chosen automatically by the networking stack are known as ephemeral ports.According to RFC 6056, the ephemeral port range should be 1024-65535.However, if we only allow port 1024-65535 for outbound traffic on the NACL of the subnets where EFS locates, we will get timeout when we try to mount the EFS from an EC2 instances from a different subnet (same AZ).Privileged source port for NFS clientFor an NFS client, it’s worth noting that the source port will not be the ephemeral port(1024-65535) as we expected for a transport connection. If we check an ESTABLISH connection for NFS, we will see the connection locates on a privileged port (a port number below 1024 and only a process with root privileges may create a socket with a privileged port):$ sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,resvport fs-xxx.efs.us-east-1.amazonaws.com:/ /mnt$ sudo netstat -tanpo | grep 2049tcp 0 0 172.31.38.122:711 172.31.35.153:2049 ESTABLISHED - keepalive (27.52/0/0)From NFS man page, it explains that NFS client use privileged port for security reason: The NFS client can choose any source port value for its sockets, but usually chooses a privileged port…As described above, the traditional default NFS authentication scheme, known as AUTH_SYS, relies on sending local UID and GID numbers to identify users making NFS requests. An NFS server assumes that if a connection comes from a privileged port, the UID and GID numbers in the NFS requests on this connection have been verified by the client’s kernel or some other local authority.The privileged port range is defined by these two kernel parameters: max_resvport and min_resvport5$ sysctl -a | grep resvportsunrpc.max_resvport = 1023sunrpc.min_resvport = 665We can see the NFS client port change if we update these two kernel parameters:$ sudo sysctl -w sunrpc.max_resvport=2048sunrpc.max_resvport = 2048$ sudo sysctl -w sunrpc.min_resvport=1024sunrpc.min_resvport = 1024$ sysctl -a | grep resvportsunrpc.max_resvport = 2048sunrpc.min_resvport = 1024$ sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,resvport fs-0705b30dbe1741990.efs.us-east-1.amazonaws.com:/ /mnt$ sudo netstat -anotp | grep 2049tcp 0 0 172.31.33.197:1306 172.31.35.153:2049 ESTABLISHED - keepalive (23.93/0/0)If we allow the outbound traffic on the NACL of the EFS’s subnets to the privileged port range of the client, the client will mount the EFS successfully.SummaryEven though we can allow the privileged port of the clients on the NACL for EFS, the configuration will be difficult and complex because: the privileged port ranges depend on the system configuration of the client side the NACL is subnet-level firewall, we need to consider other service in the same subnet if any security concern raise when allow outbound for privileged portIf no other specific business use case or concern, we can use security group on EFS to allow inbound traffic from client side without concerning the client source port.References Network File System - Wikipedia &amp;#8617; Security groups for your VPC &amp;#8617; Network ACLs &amp;#8617; Creating security groups &amp;#8617; https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt &amp;#8617; " }, { "title": "[Linux] Max open file limit with system service", "url": "/posts/max-open-file-limit-for-system-service/", "categories": "CSIE, Linux", "tags": "linux, systemd, ulimit, sudo, pam", "date": "2021-12-29 15:50:02 +0000", "snippet": "IntroductionThis article will go through an interesting configuration topics for: the max open file limit with system service (systemd daemon service) the relation between PAM (Pluggable Authentication Modules) and /etc/security/limit.conf the difference between CentOS7 and Ubuntu 20.04 on this matterScenarioWhen we want to modify the limit for max open file of the Linux process, we will configure the limit in /etc/security/limit.conf. For example, to set both the soft and hard limit to 65534 for open file (file descriptors):$ cat /etc/security/limits.conf | grep nofile* soft nofile 65534* hard nofile 65534$ sudo reboot$ ulimit -Hn65534$ ulimit -Sn65534Issue DescriptionHowever, under the configuration mentioned above (both the soft and hard limit to 65534), with AWS System Manager(SSM) Run Command function1 to execute command, we will find that the following situations for the process on a CentOS7 instance: When the command is without sudo: The soft limit will be 1024. The hard limit will be 4096. When the command is with sudo: The soft limit and hard limit will both be 65534 Why does the soft and hard limit of max opened file for process will be different if a command is executed with sudo or not?ExplanationThe scope for /etc/security/limits.confFirst, we need to know that the configuration in /etc/security/limits.conf is only effective for the users logged in via PAM:$ cat /etc/security/limits.conf# /etc/security/limits.conf##This file sets the resource limits for the users logged in via PAM.#It does not affect resource limits of the system services.And we also notice that /etc/security/limits.conf does not affect the system services (daemon service).PAM (Pluggable Authentication Modules)PAM is a module to authenticate user for the programs on Linux.2 PAM will authenticate the program (login, su etc) according to the PAM configuration in /etc/pam.d/. For instance, the PAM configuration for sudo on CentOS 7 will be:$ cat /etc/pam.d/sudo#%PAM-1.0auth include system-authaccount include system-authpassword include system-authsession optional pam_keyinit.so revokesession include system-authWe can further check the configuration system-auth included for sudo:$ cat /etc/pam.d/system-auth#%PAM-1.0# This file is auto-generated.# User changes will be destroyed the next time authconfig is run....session optional pam_keyinit.so revokesession required pam_limits.so &amp;lt;======================= Use /etc/security/limit.conf...In /etc/pam.d/system-auth, it will use a module called pam_limits.so. According to the pam_limits man page, this module will refer to /etc/security/limits.conf to set the resource limit during the user login/execute command (the period is called session): The pam_limits PAM module sets limits on the system resources that can be obtained in a user-session. Users of uid=0 are affected by this limits, too.By default limits are taken from the /etc/security/limits.conf config file…Command with sudo and without sudoTherefore, when we use SSM to execute command with sudo (authenticated through /etc/pam.d/sudo) or execute su (authenticated through /etc/pam.d/su) on a CentOS7 system, the PAM will use pam_limits.so module for authentication and thus the max open file limit of the process executed with sudo will refer to /etc/security/limits.conf, which is 65534.Without sudo the command will not go though PAM authentication, the process will inherit the limit for the parent process and the limit will be 1024 for soft limit and 4096 for hard limit in this scenario.CentOS and UbuntuWe can also notice that this behaviour cannot be reproduced on an Ubuntu system. It is because the PAM configuration for sudo on Ubuntu does not include pam_limits.so module:$ cat /etc/pam.d/sudo#%PAM-1.0session required pam_env.so readenv=1 user_readenv=0session required pam_env.so readenv=1 envfile=/etc/default/locale user_readenv=0@include common-auth@include common-account@include common-session-noninteractiveWe can check all the included PAM modules /etc/pam.d/common-auth, /etc/pam.d/common-account and /etc/pam.d/common-session-noninteractive, we will see none of them include pam_limits.so. Therefore, even though we execute command with sudo on Ubuntu, the process will not use the configuration in /etc/security/limits.conf.SolutionAs /etc/security/limits.conf does not affect the system service, we can configure the resource limit for system service by: systemd.unit (certain systemd service)3 systemd-system.conf (all services under systemd)4systemd.unitUsageIn a systemd.unit, we can configure the resource limit for certain systemd service by the following steps: Create an drop-in directory under /etc/systemd/system Create a file with extension .confWhen systemd pursers the service unit file for systemd service, systemd will also purser the configuration files in drop-in directory. In this way, we needn’t change any configuration in the service unit file, as mentioned in the systemd.unit man page: Along with a unit file foo.service, a “drop-in” directory foo.service.d/ may exist. All files with the suffix “.conf” from this directory will be parsed after the unit file itself is parsed. This is useful to alter or add configuration settings for a unit, without having to modify unit files. Drop-in files must contain appropriate section headers.ExampleTake amazon-ssm-agent daemon service for example: check the amazon-ssm-agent original max open files limit ： $ cat /proc/$(pidof amazon-ssm-agent)/limits | grep filesMax open files 1024 4096 files Create a drop-in directory： $ sudo mkdir -p /etc/systemd/system/amazon-ssm-agent.service.d/ Add a configuration file with extension .conf： $ sudo vi /etc/systemd/system/amazon-ssm-agent.service.d/filelimit.conf Add these two lines (according to the man page of systemd.service：The service specific configuration options are configured in the [Service] section)： [Service]LimitNOFILE=2048:8192 Reload systemd configuration and restart the amazon-ssm-agnet daemon service： $ sudo systemctl daemon-reload$ sudo systemctl restart amazon-ssm-agent Confirm the amazon-ssm-agent process use the max open file limit: $ cat /proc/$(pidof amazon-ssm-agent)/limits | grep filesMax open files 2048 8192 files systemd-system.confUsageIn systemd-system.conf, we can configure the limit for all the systemd service by the following step: Create a configuration file with extension .conf in /etc/systemd/system.conf.dWith the step, we needn’t to change the configuration under /etc/systemd/system.conf, as mentioned in the systemd-system.conf man page: When run as a system instance, systemd interprets the configuration file system.conf and the files in system.conf.d directories;…By default, the configuration file in /etc/systemd/ contains commented out entries showing the defaults as a guide to the administrator. This file can be edited to create local overrides.ExampleTake sshd daemon service for example: Check sshd max open files limit： $ systemctl status sshd | grep -i pid Main PID: 1596 (sshd)$ cat /proc/1596/limits | grep filesMax open files 1024 4096 files Create /etc/systemd/system.conf.d directory： $ sudo mkdir /etc/systemd/system.conf.d Create an configuration file with extension .conf： $ sudo vi /etc/systemd/system.conf.d/filelimit.conf Add the following two lines (according to systemd-system.conf man page ：All options are configured in the [Manager] section)： [Manager]DefaultLimitNOFILE=3072:12288 Reboot the system： $ sudo reboot Confirm sshd process use new max open file limit: $ systemctl status sshd | grep -i pid Main PID: 1105 (sshd)$ cat /proc/1105/limits | grep filesMax open files 3072 12288 files We can see the amazon-ssm-agent process still use its own configuration. amazon-ssm-agent will not use the systemd-wide configuration: $ cat /proc/$(pidof amazon-ssm-agent)/limits | grep filesMax open files 2048 8192 files References AWS Systems Manager Run Command &amp;#8617; PAM(8) - Linux manual page &amp;#8617; systemd.unit(5) - Linux manual page &amp;#8617; systemd-system.conf(5) - Linux manual page &amp;#8617; " }, { "title": "[Linux] Why do I run into &#39;...too long for Unix domain socket&#39; when git clone with SSH?", "url": "/posts/too-long-for-Unix-domain-socket/", "categories": "CSIE, Linux", "tags": "linux, ssh, git, socket", "date": "2021-12-28 22:00:00 +0000", "snippet": "SummaryThis article will go through why we may will run into ... too long for Unix domain socket issue when git clone with SSH after they configure the ControlMaster and ControlPath to accelerate the SSH connection.ScenarioWhen using SSH, people may want to speed up the SSH connection establishment with configuration of ControlMaster and ControlPath. According to the ssh_config man page:ControlMasterEnables the sharing of multiple sessions over a single network connection...ControlPathSpecify the path to the control socket used for connection sharing as described in the ControlMaster section above or the string &#39;&#39;none&#39;&#39; to disable connection sharing...This means that with multiple SSH session with the control socket to speed up the SSH connection establishment.IssueWhen we try to pull git repo from CodeCoommit, we may run into the following error:git clone ssh://git-codecommit.xxx.amazonaws.com/v1/repos/xxxCloning into &#39;xxx&#39;...unix_listener: &quot;/Users/xxxxxxxxxx/.ssh/master-xxx@git-codecommit.xxx:22&quot; too long for Unix domain socketfatal: Could not read from remote repository.And from the ~/.ssh/config, we can see the following configuration:host *ControlMaster autoControlPath ~/.ssh/master-%r@%h:%pExplanationUnix socket path lengthThe messages unix_listener: ... too long for Unix domain socket is from the function to create socket (source code):intunix_listener(const char *path, int backlog, int unlink_first){ struct sockaddr_un sunaddr; int saved_errno, sock; memset(&amp;amp;sunaddr, 0, sizeof(sunaddr)); sunaddr.sun_family = AF_UNIX; if (strlcpy(sunaddr.sun_path, path, sizeof(sunaddr.sun_path)) &amp;gt;= sizeof(sunaddr.sun_path)) { error(&quot;%s: \\&quot;%s\\&quot; too long for Unix domain socket&quot;, __func__, path); errno = ENAMETOOLONG; return -1; }We can see that if the socket path is longer then sizeof(sunaddr.sun_path), and then we will get the error message unix_listener: ... too long for Unix domain socket. In unix socket man page, it describes that the length of sun_path is 108 char (source code):#define UNIX_PATH_MAX 108struct sockaddr_un { sa_family_t sun_family; /* AF_UNIX */ char sun_path[UNIX_PATH_MAX]; /* pathname */};SSH config for ControlPathWhy the socket path in the use case is longer than 108 char? When configuring the ControlPath parameter as ~/.ssh/master-%r@%h:%p, according to the ssh_config man page, this means the following, : ‘%h’: target host name ‘%p’: the port ‘%r’: the remote login usernameWith AWS CodeCommit, the remote login user name is the SSH key ID, the host name is the CodeCommit endpoint1. Due to this combination, it may be longer than the 108 char and cause the error message unix_listener: ... too long for Unix domain socket:$ echo &quot;/Users/xxxxxxxxxx/.ssh/master-xxx@git-codecommit.xxx:22&quot; | wc -m115SolutionTo solve the long ControlPath issue, we can replace the parameter with %C which is Hash of %l%h%p%r2 and decrease the chance for ControlPath to be longer than 108 char:host *ControlMaster autoControlPath ~/.ssh/master-%CWe can see the length of the socket path is much less then the length limit and we can avoid the socket length issue with ControlPath:$ ls ~/.ssh | grep mastermaster-e7f58eb55da9db1a90b39dba271ef9c92838e09a$ echo &quot;/home/shihtiy/.ssh/master-e7f58eb55da9db1a90b39dba271ef9c92838e09a&quot; | wc -m67References Setup steps for SSH connections to AWS CodeCommit repositories on Linux, macOS, or Unix  &amp;#8617; ssh_config(5) - OpenBSD manual pages &amp;#8617; " } ]
